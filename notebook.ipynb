{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f6220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/pipeline/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f36b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/pipeline/categories.py\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "class Categorify(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, top_n: int = None,\n",
    "                 add_nan: bool = False,\n",
    "                 dtype=np.int64):\n",
    "        self.top_n = top_n\n",
    "        self.add_nan = add_nan\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X: pd.Series):\n",
    "        if not isinstance(X, pd.Series):\n",
    "            X = pd.Series(X)\n",
    "        has_nan = X.isna().sum() > 0\n",
    "        self.add_nan = (self.add_nan and has_nan)\n",
    "        categories = (list(X.categories)\n",
    "                      if is_categorical_dtype(X)\n",
    "                      else sorted(list(f for f in X.drop_duplicates()\n",
    "                                       if not pd.isna(f))))\n",
    "        if self.top_n is not None:\n",
    "            counter = Counter(X)\n",
    "            top_categories = [cat for cat, _ in\n",
    "                              counter.most_common(self.top_n)]\n",
    "            categories = [cat for cat in categories\n",
    "                          if cat in top_categories]\n",
    "        self.categories = categories\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        Xcat = pd.Categorical(X, categories=self.categories, ordered=True)\n",
    "        Xcodes = Xcat.codes.astype(self.dtype)\n",
    "        if self.add_nan:\n",
    "            Xcodes += 1\n",
    "        assert Xcodes.min() == 0, f'the min is {Xcodes.min()}'\n",
    "        return pd.DataFrame({X.name: Xcodes})\n",
    "\n",
    "\n",
    "class PdKBinsDiscretizer(KBinsDiscretizer):\n",
    "    def __init__(self, n_bins=5,\n",
    "                 encode: str = 'ordinal',\n",
    "                 strategy: str ='quantile'):\n",
    "        super().__init__(n_bins=n_bins,\n",
    "                 encode=encode,\n",
    "                 strategy=strategy)\n",
    "        \n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        features = list(X.columns)\n",
    "        outputX = super().transform(X).astype(np.int64)\n",
    "        return pd.DataFrame(outputX, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382f2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/pipeline/continuous.py\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pandas.api.types import is_float_dtype\n",
    "\n",
    "\n",
    "class MedianFillNaN(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features: List[str] = None):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        if self.features is not None:\n",
    "            df = df.loc[:, self.features]\n",
    "        self.medians = df.median().to_dict()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        return X.fillna(self.medians)\n",
    "\n",
    "\n",
    "\n",
    "class FilterContinuousFeatures(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, ignore_features: List[str]):\n",
    "        self.ignore_features = ignore_features\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        self._features = [name for name, values in df.items()\n",
    "                          if (is_float_dtype(values) and\n",
    "                              name not in self.ignore_features)]\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:        \n",
    "        return df.loc[:, self._features]\n",
    "\n",
    "\n",
    "\n",
    "class PdStandardNorm(BaseEstimator, TransformerMixin):\n",
    "    eps = 1e-15\n",
    "\n",
    "    def fit(self, X: pd.DataFrame , y=None):\n",
    "        self.means = X.mean()\n",
    "        self.stds = X.std() + self.eps\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        output = (X - self.means) / self.stds\n",
    "        return output.astype(np.float32)\n",
    "\n",
    "\n",
    "class PdScaleNorm(BaseEstimator, TransformerMixin):\n",
    "    eps = 1e-15\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self.min = X.min()\n",
    "        self.max = X.max()\n",
    "        self.difference = self.max - self.min + self.eps\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        output = (X - self.min) / self.difference\n",
    "        return output.astype(np.float16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd1ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/pipeline/core.py\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union, List, Tuple\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def pandas_hstack(Xs):\n",
    "    return pd.concat([X.reset_index(drop=True) for X in Xs], axis=1)\n",
    "\n",
    "\n",
    "def reduce_fn(left: pd.DataFrame,\n",
    "              right: pd.DataFrame):\n",
    "    common_cols = [f for f in left.columns\n",
    "                   if f in right.columns]\n",
    "    if len(common_cols) > 0:\n",
    "        return left.merge(right, on=common_cols, how='left')\n",
    "    return pd.concat([left, right], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "class CoreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X: pd.DataFrame, y=np.ndarray):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        return X\n",
    "\n",
    "\n",
    "class PdColumnTransformer(ColumnTransformer):\n",
    "    def _hstack(self, Xs):\n",
    "        return reduce(reduce_fn, Xs)\n",
    "\n",
    "\n",
    "class PdFeatureUnion(FeatureUnion):\n",
    "    def _hstack(self, Xs):\n",
    "        return reduce(reduce_fn, Xs)\n",
    "\n",
    "\n",
    "def unpack_json(json_str):\n",
    "    return np.nan if pd.isna(json_str) else pd.read_json(json_str)\n",
    "\n",
    "\n",
    "def unpack_column(series: pd.DataFrame) -> pd.DataFrame:\n",
    "    def _unpack_row(index, row):\n",
    "        out_df = unpack_json(row)\n",
    "        out_df = out_df.assign(date=index)\n",
    "        return out_df\n",
    "    \n",
    "    return pd.concat([_unpack_row(index, row)\n",
    "                      for index, row in series.iteritems()\n",
    "                      if pd.notna(row)], ignore_index=True)\n",
    "\n",
    "\n",
    "def make_column_tmf(*transformers: List[Tuple[TransformerMixin, Union[List[str], str]]],\n",
    "                    **kwargs):\n",
    "    names = [type(tmf).__name__ for (tmf, _) in transformers]\n",
    "    return PdColumnTransformer([(name, tmf, features)\n",
    "                                 for name, (tmf, features) in zip(names, transformers)], **kwargs)\n",
    "\n",
    "\n",
    "def make_unpack_tmf(column_name: str):\n",
    "    return make_column_tmf((FunctionTransformer(unpack_column), column_name))\n",
    "\n",
    "\n",
    "def forward_fill(df: pd.DataFrame, features: List[str],\n",
    "                 on='playerId',\n",
    "                 suffix: str = None,\n",
    "                 limit: int =None):\n",
    "    ffilled_df = df.groupby(on)[features].ffill(limit=limit)\n",
    "    output_features = (features if suffix is None else\n",
    "                       list(map(lambda f: suffix + f, features)))\n",
    "    outputX = df.copy()\n",
    "    outputX.loc[:, output_features] = ffilled_df.to_numpy()\n",
    "    return outputX\n",
    "\n",
    "\n",
    "def gen_hardcoded_features(df: pd.DataFrame):\n",
    "    # some feature eng for the dates\n",
    "    df['pstatsTime'] = (df['date'] - pd.to_datetime(df['pstatsDate'])).dt.total_seconds()\n",
    "    df['playerAge'] = (df['date'] - df['DOB']).dt.total_seconds()\n",
    "    df['playerTSinceDebut'] = (df['date'] - df['mlbDebutDate']).dt.total_seconds()\n",
    "    df['playerDebutAge'] = (df['mlbDebutDate'] - df['DOB']).dt.total_seconds()\n",
    "    df['rostersTime'] = (df['date'] - pd.to_datetime(df['rosterDate'])).dt.total_seconds()\n",
    "    df[['rostersTime', 'pstatsTime']] /= 60 * 60 * 24\n",
    "    # normalize\n",
    "    df[['playerAge', 'playerTSinceDebut', 'playerDebutAge']] /= 60 * 60 * 24 * 365\n",
    "    return df\n",
    "\n",
    "def fillna(df: pd.DataFrame, fill_value=0):\n",
    "    return df.fillna(fill_value)\n",
    "\n",
    "\n",
    "class FilterFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, suffixes: List[str]):\n",
    "        self.suffixes = suffixes\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self._features = list(filter(lambda f: any([f.endswith(suffix) for suffix in self.suffixes]),\n",
    "                                     X.columns))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        return X.loc[:, self._features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cceff548",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/pipeline/players.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def player_preprocessing(players: pd.DataFrame) -> pd.DataFrame:    \n",
    "    # weight to kg and height to cm\n",
    "    players['weight'] *= 0.453592\n",
    "    players['height'] = players['heightInches'] * 2.54 / 100\n",
    "    players['playerBMI'] = players['weight'] / np.power(players['height'], 2)\n",
    "    \n",
    "    # drop unnecessary features\n",
    "    to_drop = ['birthCity', 'heightInches',\n",
    "               'birthStateProvince',\n",
    "               'primaryPositionCode',\n",
    "               'playerName',\n",
    "               'playerForTestSetAndFuturePreds']\n",
    "    players.drop(to_drop, inplace=True, axis=1)\n",
    "    return players\n",
    "\n",
    "\n",
    "def join_players_info(df: pd.DataFrame, path_to_players_csv: str):\n",
    "    # read players csv\n",
    "    raw_players = pd.read_csv(path_to_players_csv, parse_dates=['DOB', 'mlbDebutDate'])\n",
    "    # process player data\n",
    "    players = player_preprocessing(raw_players)\n",
    "    # merge\n",
    "    return df.merge(players, how='left', on=['playerId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6afa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/pipeline/season.py\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def date_preprocessing(df: pd.DataFrame,\n",
    "                       path_to_season: str,\n",
    "                       dt_col: str = 'date',\n",
    "                       date_attr: List[str] = ['year', 'month']):\n",
    "    assert 'year' in date_attr, \\\n",
    "            'year attr must be on the date_attr list'\n",
    "    date_cols = ['seasonStartDate', 'seasonEndDate', 'preSeasonStartDate',\n",
    "                 'preSeasonEndDate', 'regularSeasonStartDate', 'regularSeasonEndDate',\n",
    "                 'lastDate1stHalf', 'allStarDate', 'firstDate2ndHalf',\n",
    "                 'postSeasonStartDate', 'postSeasonEndDate']\n",
    "    seasons = pd.read_csv(path_to_season, parse_dates=date_cols)\n",
    "    \n",
    "    # adding date attr\n",
    "    for attr in date_attr:\n",
    "        attr = attr.lower()\n",
    "        df[attr] = getattr(df[dt_col].dt, attr)\n",
    "\n",
    "    season_df = pd.merge(df, seasons, left_on='year', right_on='seasonId')\n",
    "\n",
    "    season_df['inSeason'] = (season_df['date'].between(\n",
    "                                    season_df['regularSeasonStartDate'],\n",
    "                                    season_df['postSeasonEndDate'],\n",
    "                                    inclusive = True\n",
    "                                    )\n",
    "                                  )\n",
    "\n",
    "    season_df['seasonPart'] = np.select(\n",
    "      [\n",
    "        season_df['date'] < season_df['preSeasonStartDate'], \n",
    "        season_df['date'] < season_df['regularSeasonStartDate'],\n",
    "        season_df['date'] <= season_df['lastDate1stHalf'],\n",
    "        season_df['date'] < season_df['firstDate2ndHalf'],\n",
    "        season_df['date'] <= season_df['regularSeasonEndDate'],\n",
    "        season_df['date'] < season_df['postSeasonStartDate'],\n",
    "        season_df['date'] <= season_df['postSeasonEndDate'],\n",
    "        season_df['date'] > season_df['postSeasonEndDate']\n",
    "      ], \n",
    "      [\n",
    "        'Offseason',\n",
    "        'Preseason',\n",
    "        'Reg Season 1st Half',\n",
    "        'All-Star Break',\n",
    "        'Reg Season 2nd Half',\n",
    "        'Between Reg and Postseason',\n",
    "        'Postseason',\n",
    "        'Offseason'\n",
    "      ], \n",
    "      default = np.nan\n",
    "      )\n",
    "\n",
    "    season_df.drop(seasons.columns, axis=1, inplace=True)\n",
    "    \n",
    "    return season_df\n",
    "  \n",
    "\n",
    "def join_season_info(df: pd.DataFrame,\n",
    "                    path_to_season_csv: str,\n",
    "                    date_attr: List[str] = ['year', 'weekday'],\n",
    "                    ):\n",
    "    # get unique dates\n",
    "    dates = df[['date']].drop_duplicates()\n",
    "    # add season info\n",
    "    dates = date_preprocessing(dates, path_to_season_csv,\n",
    "                   date_attr=date_attr)\n",
    "    # merge\n",
    "    return df.merge(dates, how='left', on=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397d65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/pipeline/stats.py\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StatisticTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self._features = list(X.columns.drop(self.index_cols))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        X.set_index('date', inplace=True)\n",
    "        output = X.groupby(self.ids)[self._features].apply(self.compute_features)\n",
    "        output.reset_index(inplace=True)\n",
    "        \n",
    "        X.reset_index(inplace=True)\n",
    "        \n",
    "        assert np.all(X[self.index_cols] == output[self.index_cols]), \\\n",
    "               'the ids do not match!'\n",
    "        if self.drop_index:\n",
    "            output.drop(self.index_cols, axis=1, inplace=True)\n",
    "        return output\n",
    "\n",
    "\n",
    "class StatisticGen(StatisticTransformer):\n",
    "    def __init__(self, stats: List[str] = ['mean'],\n",
    "                 windows: List[int] = [10],\n",
    "                 dt_col: str = 'date',\n",
    "                 ids: List[str] = ['playerId'],\n",
    "                 drop_index: bool = True):        \n",
    "        self.stats = stats\n",
    "        self.windows = windows\n",
    "        self.dt_col = dt_col\n",
    "        self.ids = ids\n",
    "        self.index_cols = self.ids + [self.dt_col]\n",
    "        self.drop_index = drop_index\n",
    "        \n",
    "    def _compute_features(self, df: pd.DataFrame, window: int) -> pd.DataFrame:\n",
    "        stats_df = df.rolling(window, min_periods=1).agg(self.stats)\n",
    "        stats_df.columns = ['__'.join(list(f) + [f'{window}d'])\n",
    "                            for f in stats_df]\n",
    "        return stats_df.astype(np.float32)\n",
    "    \n",
    "    def compute_features(self, df: pd.DataFrame):\n",
    "        return pd.concat([self._compute_features(df, window)\n",
    "                               for window in self.windows], axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "class LagGen(StatisticTransformer):\n",
    "    def __init__(self, lags: List[int] = [10],\n",
    "                 dt_col: str = 'date',\n",
    "                 ids: List[str] = ['playerId'],\n",
    "                 drop_index: bool = True):\n",
    "        self.lags = lags\n",
    "        self.dt_col = dt_col\n",
    "        self.ids = ids\n",
    "        self.index_cols = self.ids + [self.dt_col]\n",
    "        self.drop_index = drop_index\n",
    "        \n",
    "    def _compute_features(self, df: pd.DataFrame, lag: int) -> pd.DataFrame:\n",
    "        lagdf = df.shift(lag)\n",
    "        lagdf.columns = [f'{f}__{lag}lag' for f in lagdf.columns]\n",
    "        return lagdf.astype(np.float32)\n",
    "        \n",
    "    def compute_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.concat([self._compute_features(df, lag)\n",
    "                          for lag in self.lags], axis=1) \n",
    "\n",
    "\n",
    "class FeaturesTable(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, table: pd.DataFrame,\n",
    "                 lags: List[int],\n",
    "                 on: List[str] = ['playerId', 'date'],\n",
    "                 date_col: str = 'date',\n",
    "                 clip_max: bool = True):\n",
    "        self.on = on\n",
    "        self.lags = lags\n",
    "        self.table = table\n",
    "        self.date_col = date_col\n",
    "        self.clip_max = clip_max\n",
    "        self.max_date = table[self.date_col].max().to_numpy()\n",
    "\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        return self\n",
    "\n",
    "    def _merge_table(self, X: pd.DataFrame,\n",
    "                     lag: int) -> pd.DataFrame:\n",
    "        Xon = X.loc[:, self.on].copy()\n",
    "        Xon[self.date_col] = X[self.date_col] - pd.to_timedelta(lag, unit='d')\n",
    "        if self.clip_max:\n",
    "            Xon[self.date_col] = np.minimum(Xon[self.date_col], self.max_date)\n",
    "        outputX = Xon.merge(self.table, on=self.on,\n",
    "                            how='left').drop(self.on, axis=1)\n",
    "        assert len(outputX) == len(X), \\\n",
    "               f'the len {len(X)} of the input do not match the output len {len(outputX)}'\n",
    "        outputX.columns += f'__{lag}lag'\n",
    "        return outputX\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        return pd.concat([self._merge_table(X, lag)\n",
    "                          for lag in self.lags], axis=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb689a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/pipeline/target.py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def target_preprocessing(target: pd.DataFrame) -> pd.DataFrame:\n",
    "    target.drop('engagementMetricsDate', axis=1, inplace=True)\n",
    "    target.sort_values(by=['playerId', 'date'], inplace=True)\n",
    "    target.reset_index(drop=True, inplace=True)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c8da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe5c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/dataset.py\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "def get_timeseries_index(values: np.ndarray,\n",
    "                         idx: int,\n",
    "                         bptt: int):\n",
    "\n",
    "    start_idx = max(0, idx - bptt)\n",
    "    timeft = values[start_idx: idx + 1]\n",
    "    \n",
    "    if len(timeft) <= bptt:\n",
    "        missing_steps = bptt - len(timeft) + 1\n",
    "        timeft = np.pad(timeft, pad_width=((missing_steps, 0), (0, 0)))\n",
    "\n",
    "    return timeft\n",
    "\n",
    "\n",
    "\n",
    "class PlayerDataset:\n",
    "    def __init__(self, features: np.ndarray,\n",
    "                 categories: np.ndarray,\n",
    "                 target: np.ndarray = None,\n",
    "                 bptt: int = 10,\n",
    "                 weight: np.ndarray = None,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.bptt = bptt - 1\n",
    "        self.device = device\n",
    "        self.categories = categories\n",
    "        self.weight = weight\n",
    "        assert len(self) > self.bptt, f'lenght: {len(self)}, bptts: {self.bptt}'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def _to_torch(self, array):\n",
    "        return torch.from_numpy(np.asarray(array)).to(device=self.device)\n",
    "    \n",
    "    def to_device(self, batch_item: Dict[str, np.ndarray]):\n",
    "        return {name: self._to_torch(values)\n",
    "                for name, values in batch_item.items()}\n",
    "    \n",
    "    def __getitem__(self, idx: int):        \n",
    "        features = (get_timeseries_index(self.features, idx=idx,\n",
    "                                         bptt=self.bptt)\n",
    "                    if self.bptt > 0 else self.features[idx])\n",
    "        cats = (get_timeseries_index(self.categories, idx=idx,\n",
    "                                         bptt=self.bptt)\n",
    "                    if self.bptt > 0 else self.categories[idx])\n",
    "\n",
    "        batch_item = {'features': features.astype(np.float32),\n",
    "                      'categories': cats.astype(np.int64)}\n",
    "        if self.target is not None:\n",
    "            batch_item['target'] = self.target[idx]\n",
    "        if self.weight is not None:\n",
    "            batch_item['weight'] = self.weight[idx]\n",
    "    \n",
    "        return self.to_device(batch_item)\n",
    "\n",
    "    @classmethod\n",
    "    def from_df(cls, df: pd.DataFrame,\n",
    "                features: List[str],\n",
    "                categories: List[str],\n",
    "                target: List[str]=None,\n",
    "                weight: List[str]=None,\n",
    "                bptt: int = 30,\n",
    "                device: torch.device = torch.device('cpu')):\n",
    "\n",
    "        if target is not None:\n",
    "            target = df.loc[:, target].to_numpy()\n",
    "        if weight is not None:\n",
    "            weight = df.loc[:, weight].to_numpy()\n",
    "    \n",
    "        return cls(features=df.loc[:, features].to_numpy(),\n",
    "                categories=df.loc[:, categories].to_numpy(),\n",
    "                target=target,\n",
    "                weight=weight,\n",
    "                bptt=bptt,\n",
    "                device=device)\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.features, self.target, self.categories\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd7bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/split.py\n",
    "from datetime import datetime\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "class SplitData:\n",
    "    \"\"\"Helper class to split the data for time series\"\"\"\n",
    "    def __init__(self, date: Union[datetime, str],\n",
    "                 train_days: int = None,\n",
    "                 test_days: int = 31,\n",
    "                 gap: int = 0,\n",
    "                 dt_col: str = 'date'):\n",
    "        if isinstance(date, str):\n",
    "            date = pd.to_datetime(date)\n",
    "        # test range\n",
    "        self.start_test_dt = date\n",
    "        self.end_test_dt =  date + pd.to_timedelta(test_days, unit='d')\n",
    "        # train range\n",
    "        self.end_train_dt = date - pd.to_timedelta(gap, unit='d')\n",
    "        self.start_train_dt = (self.end_train_dt - pd.to_timedelta(train_days, unit='d')\n",
    "                               if train_days is not None else\n",
    "                               None)\n",
    "        self.dt_col = dt_col\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_index(dates: pd.Series,\n",
    "                start: datetime,\n",
    "                end: datetime):\n",
    "        index = (dates >= start) & (dates < end)\n",
    "        return index\n",
    "    \n",
    "    def train_idx(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        start_train_dt = (df[self.dt_col].min()\n",
    "                          if self.start_train_dt is None\n",
    "                          else self.start_train_dt)\n",
    "        index = self._get_index(df[self.dt_col], start_train_dt,\n",
    "                                self.end_train_dt)\n",
    "        return index\n",
    "\n",
    "    def valid_idx(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        index = self._get_index(df[self.dt_col], self.start_test_dt,\n",
    "                                self.end_test_dt)\n",
    "        return index\n",
    "    \n",
    "    def train(self, df: pd.DataFrame):\n",
    "        return self.filter(df, self.train_idx(df))\n",
    "    \n",
    "    def valid(self, df: pd.DataFrame):\n",
    "        return self.filter(df, self.valid_idx(df))\n",
    "    \n",
    "    def filter(self, df: pd.DataFrame, index):\n",
    "        return df.loc[index, :].reset_index(drop=True)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f'test_range=({self.start_test_dt}, {self.end_test_dt}), '\n",
    "                f'train_range=({self.start_train_dt}, {self.end_train_dt})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c14b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/util.py\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def filter_by_id(df: pd.DataFrame, ids: List[int]) -> pd.DataFrame:\n",
    "    index = df['playerId'].isin(ids)\n",
    "    return df.loc[index, :].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "def sample_by_id(df: pd.DataFrame, n: int = 1) -> pd.DataFrame:\n",
    "    unique_id = df['playerId'].unique()\n",
    "    choosen_id = np.random.choice(unique_id, n, replace=False)\n",
    "    return  filter_by_id(df, choosen_id)\n",
    "\n",
    "\n",
    "def filter_by_date(df: pd.DataFrame,\n",
    "                   date: str,\n",
    "                   dt_col: str = 'date') -> pd.DataFrame:\n",
    "    index = df.loc[:, dt_col] >= date\n",
    "    return df.loc[index, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "110a268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/ingest_data/__init__.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "feature_fields = {'games': 'games', \n",
    "                   'playerBoxScores': 'pstats',\n",
    "                   'rosters': 'rosters',\n",
    "                   'playerTwitterFollowers': 'ptw_fl',\n",
    "                   'teamTwitterFollowers': 'team_tw_fl', \n",
    "                   'awards': 'awards',\n",
    "                   'teamBoxScores': 'teams',\n",
    "                   'standings': 'standings'}\n",
    "\n",
    "\n",
    "def ingest_target(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print('preprocessing target')\n",
    "    fields = unpack_dataframe(df, fields={'nextDayPlayerEngagement': 'target'})\n",
    "    target = fields['target']\n",
    "    return target_preprocessing(target)\n",
    "\n",
    "\n",
    "def ingest_stats_features(pstats: pd.DataFrame,\n",
    "                          games: pd.DataFrame,\n",
    "                          teams: pd.DataFrame):\n",
    "    teams = ingest_team_stats(teams)\n",
    "    games = ingest_games_stats(games)\n",
    "    pstats = ingest_player_stats(pstats)\n",
    "    pstats = join_games_stats_to_pstats(pstats, games)\n",
    "    pstats = join_team_stats_to_pstats(pstats, teams)\n",
    "    pstats.drop(['gamePk', 'teamId'], axis=1, inplace=True)\n",
    "    return pstats\n",
    "\n",
    "\n",
    "def ingest_rosters(rosters: pd.DataFrame):\n",
    "    # rename roster date\n",
    "    rosters.rename(columns={'gameDate': 'rosterDate'}, inplace=True)\n",
    "    # drop statusCode\n",
    "    rosters.drop(['statusCode'], axis=1, inplace=True)\n",
    "    assert not has_duplicates(rosters), 'rosters include duplicates'\n",
    "    return rosters\n",
    "\n",
    "\n",
    "def ingest_player_twitter_fl(tw_fl: pd.DataFrame):\n",
    "    tw_fl = tw_fl.loc[:, ['playerId', 'numberOfFollowers', 'date']]\n",
    "    assert not has_duplicates(tw_fl), 'player tw include duplicates'\n",
    "    return tw_fl\n",
    "\n",
    "\n",
    "def ingest_team_twitter_fl(tw_fl: pd.DataFrame):\n",
    "    tw_fl.rename(columns={'numberOfFollowers': 'teamFollowers'}, inplace=True)\n",
    "    tw_fl = tw_fl.loc[:, ['teamId', 'teamFollowers', 'date']]\n",
    "    assert not has_duplicates(tw_fl, on=['date', 'teamId']), 'team tw include duplicates'\n",
    "    return tw_fl\n",
    "\n",
    "\n",
    "def ingest_awards(awards: pd.DataFrame):\n",
    "    awards = awards[['date', 'awardId', 'playerId']]\n",
    "    awards = awards.groupby(['date', 'playerId'])[['awardId']].count()\n",
    "    awards.rename(columns={'awardId': 'awardCount'}, inplace=True)\n",
    "    awards.reset_index(inplace=True)\n",
    "    # awards['totalAwardCount'] = awards.groupby(['playerId'])[['awardId']].count()\n",
    "    assert not has_duplicates(awards), 'awards tw include duplicates'\n",
    "    return awards\n",
    "\n",
    "\n",
    "\n",
    "def ingest_features(df: pd.DataFrame,\n",
    "                    pstats: pd.DataFrame = None,\n",
    "                    games: pd.DataFrame = None,\n",
    "                    awards: pd.DataFrame = None,\n",
    "                    ptw_fl: pd.DataFrame = None,\n",
    "                    rosters: pd.DataFrame = None,\n",
    "                    standings: pd.DataFrame = None,\n",
    "                    team_tw_fl: pd.DataFrame = None,\n",
    "                    teams: pd.DataFrame = None,\n",
    "                    path_to_players_csv: str = None,\n",
    "                    path_to_season_csv: str = None):\n",
    "\n",
    "\n",
    "    if standings is not None:\n",
    "        standings = ingest_standings(standings)\n",
    "    if (pstats is not None) and (games is not None):\n",
    "        pstats = ingest_stats_features(pstats, games, teams)\n",
    "    if rosters is not None:\n",
    "        rosters = ingest_rosters(rosters)\n",
    "    if (awards is not None):\n",
    "        awards = ingest_awards(awards)\n",
    "    if ptw_fl is not None:\n",
    "        ptw_fl = ingest_player_twitter_fl(ptw_fl)\n",
    "    if team_tw_fl is not None:\n",
    "        team_tw_fl = ingest_team_twitter_fl(team_tw_fl)\n",
    "    \n",
    "    for feature_ds in [pstats, rosters, ptw_fl, awards]:\n",
    "        if feature_ds is None:\n",
    "            continue\n",
    "        df = df.merge(feature_ds, on=['playerId', 'date'],\n",
    "                      how='left')\n",
    "\n",
    "    if (team_tw_fl is not None) and ('teamId' in df.columns):\n",
    "        df = df.merge(team_tw_fl, on=['teamId', 'date'],\n",
    "                    how='left')\n",
    "\n",
    "    if (standings is not None) and ('teamId' in df.columns):\n",
    "        df = join_standings_to(df, standings)\n",
    "\n",
    "    if path_to_season_csv is not None:\n",
    "        df = join_season_info(df, path_to_season_csv=path_to_season_csv)\n",
    "    if path_to_players_csv is not None:\n",
    "        df = join_players_info(df, path_to_players_csv=path_to_players_csv)\n",
    "    assert not has_duplicates(df), 'output features include duplicates'\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_test_template(sample_submission: pd.DataFrame):\n",
    "    test_df = sample_submission.copy(deep=True)\n",
    "    test_df.reset_index(inplace=True)\n",
    "    test_df['date'] = pd.to_datetime(test_df['date'], format='%Y%m%d')\n",
    "    \n",
    "    test_df.sort_values(by=['playerId', 'date'], inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    return test_df.drop('date_playerId', axis=1)\n",
    "\n",
    "\n",
    "def ingest_features_for_test(test_df: pd.DataFrame,\n",
    "                             raw_test_df: pd.DataFrame,\n",
    "                             train_fields: Dict[str, pd.DataFrame], \n",
    "                             path_to_players_csv: str = None,\n",
    "                             path_to_season_csv: str = None):\n",
    "    # read each feature dataset\n",
    "    test_fields = unpack_dataframe(raw_test_df, fields=feature_fields)\n",
    "    # update the information in the train fields\n",
    "    test_fields = update_fields(train_fields, test_fields)\n",
    "    # compute all features\n",
    "    test_df = ingest_features(test_df, **test_fields,\n",
    "                              path_to_players_csv=path_to_players_csv,\n",
    "                              path_to_season_csv=path_to_season_csv)\n",
    "    return test_df, test_fields\n",
    "\n",
    "\n",
    "def ingest_test_data(submission_template: pd.DataFrame,\n",
    "                     raw_test_df: pd.DataFrame,\n",
    "                     train_fields: Dict[str, pd.DataFrame],\n",
    "                     path_to_players_csv: str = None,\n",
    "                     path_to_season_csv: str = None):\n",
    "    submission_template['playerId'] = (submission_template['date_playerId']\n",
    "                                      .map(lambda x: int(x.split('_')[1])))\n",
    "    test_df = create_test_template(submission_template)\n",
    "\n",
    "    test_date = test_df['date'].iloc[0]\n",
    "    raw_test_df['date'] = test_date\n",
    "    raw_test_df.set_index('date', inplace=True)\n",
    "\n",
    "    test_df, test_fields = ingest_features_for_test(test_df, raw_test_df,\n",
    "                                       train_fields,\n",
    "                                       path_to_players_csv=path_to_players_csv,\n",
    "                                       path_to_season_csv=path_to_season_csv) \n",
    "    return submission_template, test_df, test_fields\n",
    "\n",
    "\n",
    "def ingest_train_data(path_to_train_csv: str,\n",
    "                      path_to_players_csv: str,\n",
    "                      path_to_season_csv: str) -> pd.DataFrame:\n",
    "    print('reading training data..')\n",
    "    train_data = pd.read_csv(path_to_train_csv, parse_dates=['date'])\n",
    "    # set index date\n",
    "    train_data = train_data.set_index('date')\n",
    "    # ingest target data\n",
    "    df = ingest_target(train_data)\n",
    "\n",
    "    feature_fields_data = unpack_dataframe(train_data, fields=feature_fields)\n",
    "    df = ingest_features(df, **feature_fields_data,\n",
    "                        path_to_players_csv=path_to_players_csv,\n",
    "                        path_to_season_csv=path_to_season_csv)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fea4f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/ingest_data/core.py\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "fields_type = Dict[str, pd.DataFrame] \n",
    "\n",
    "def unpack_dataframe(df: pd.DataFrame, fields: List[str]) -> fields_type:\n",
    "    output = {}\n",
    "    for field, output_name in fields.items():\n",
    "        # check if there is data for this feature\n",
    "        if df.loc[:, field].isna().all():\n",
    "            output[output_name] = None\n",
    "            continue\n",
    "        # unpack data\n",
    "        tmf = make_unpack_tmf(field)\n",
    "        field_data = tmf.fit_transform(df)\n",
    "        output[output_name] = field_data\n",
    "    return output\n",
    "\n",
    "\n",
    "def update_fields(train_fields: fields_type,\n",
    "                  test_fields: fields_type,\n",
    "                  add_field: bool = False,\n",
    "                  concat: bool = True) -> fields_type:\n",
    "\n",
    "    for field, test_data in test_fields.items():\n",
    "        # if the field in the main dict, and if there is some data to add\n",
    "        if (field in train_fields):\n",
    "            train_data = train_fields[field]\n",
    "            updated_data = (train_data.append(test_data, ignore_index=True)\n",
    "                            if concat else test_data)\n",
    "        elif not (field in train_fields) and add_field:\n",
    "            updated_data = test_data\n",
    "        train_fields[field] = updated_data\n",
    "    return train_fields\n",
    "\n",
    "\n",
    "def split_fields_by_date(fields: fields_type,\n",
    "                 start_date: str = None,\n",
    "                 end_date: str = None,\n",
    "                 features: List[str] = None,\n",
    "                 dt_col: str = 'date'):\n",
    "    if features is None:\n",
    "        features = list(fields.keys())\n",
    "\n",
    "    for feature in features:\n",
    "        data = fields[feature]\n",
    "        if start_date is not None:\n",
    "            index = (data[dt_col] >= start_date)\n",
    "            if index.sum() == 0:\n",
    "                print(feature, index.sum())\n",
    "            else:\n",
    "                data = data.loc[index, :]\n",
    "        if end_date is not None:\n",
    "            index = (data[dt_col] < start_date)\n",
    "            assert index.sum() > 0\n",
    "            data = data.loc[index, :]\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        fields[feature] = data\n",
    "    return fields\n",
    "\n",
    "    \n",
    "def has_duplicates(X: pd.DataFrame,\n",
    "                   on: List[str] = ['playerId', 'date']) -> bool:\n",
    "    return X.loc[:, on].duplicated().sum() > 0\n",
    "\n",
    "\n",
    "def compute_rank_features(df: pd.DataFrame, on: List[str],\n",
    "                  features: List[str]):\n",
    "    suffix = '__' + '__'.join(on + ['ranked'])\n",
    "    output_features = list(pd.Series(features) + suffix)\n",
    "    ranked_features = df.groupby(on)[features].rank()\n",
    "    df.loc[:, output_features] = ranked_features.fillna(0).to_numpy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_with_max(df: pd.DataFrame,\n",
    "                       on: List[str],\n",
    "                       features: List[str]):\n",
    "    maximum = df.groupby(on)[features].transform('max')\n",
    "    output_features = [f'__'.join([f] + on + ['maxNorm'])\n",
    "                       for f in features]\n",
    "    normalized_features = df.loc[:, features] / maximum.to_numpy()\n",
    "    df.loc[:, output_features] = normalized_features.to_numpy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_suffix(df: pd.DataFrame, features: List[str],\n",
    "               suffix: str):\n",
    "    new_features_names = {name: f'{name}__{suffix}'\n",
    "                          for name in features}\n",
    "    return df.rename(columns=new_features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df861839",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/ingest_data/games.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_games_stats(games: pd.DataFrame):    \n",
    "    to_drop = ['gameTimeUTC', 'resumeDate',\n",
    "               'resumedFrom', 'codedGameState',\n",
    "               'detailedGameState',\n",
    "               'gameNumber',\n",
    "               'doubleHeader',\n",
    "               'dayNight',\n",
    "               'scheduledInnings', \n",
    "               'homeName',\n",
    "               'homeAbbrev',\n",
    "               'gameType',\n",
    "               'homeWins',\n",
    "               'homeLosses',\n",
    "               'homeWinPct',\n",
    "               'awayWins',\n",
    "               'awayLosses',\n",
    "               'awayWinPct',\n",
    "               'homeName',\n",
    "               'gameDate',\n",
    "               'awayName', 'awayAbbrev', 'isTie']\n",
    "    games['gamesInSeries'] = (games['gamesInSeries'].replace({0: 1})\n",
    "                              .fillna(1).astype(np.int64))\n",
    "    # drop features\n",
    "    \n",
    "    games.sort_values(by=['date', 'gamePk'], inplace=True)\n",
    "    games.reset_index(drop=True, inplace=True)\n",
    "    return games.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "def _join_home_and_away_games(games: pd.DataFrame):\n",
    "    # home games\n",
    "    home_team_games = games.copy()\n",
    "    home_team_games['home'] = 1\n",
    "    home_team_games['away'] = 0\n",
    "    home_team_games.rename(columns={'homeId': 'teamA',\n",
    "                                    'awayId': 'teamB'}, inplace=True)\n",
    "\n",
    "    # away games\n",
    "    away_team_games = games.copy()\n",
    "    away_team_games['home'] = 0\n",
    "    away_team_games['away'] = 1\n",
    "    away_team_games.rename(columns={'homeId': 'teamB',\n",
    "                                    'awayId': 'teamA'}, inplace=True)\n",
    "    \n",
    "    team_games = pd.concat([home_team_games, away_team_games],\n",
    "                               axis=0, ignore_index=True)\n",
    "    \n",
    "    # sort values\n",
    "    team_games = (team_games.sort_values(by=['teamA', 'teamB', 'date'])\n",
    "                  .reset_index(drop=True))\n",
    "    \n",
    "    return team_games\n",
    "\n",
    "\n",
    "def compute_current_game_in_series(games: pd.DataFrame):\n",
    "    team_games = _join_home_and_away_games(games)\n",
    "    \n",
    "    features = ['teamA', 'teamB', 'date', 'season', 'seriesDescription']\n",
    "    team_games = team_games.loc[:, features]\n",
    "    \n",
    "    team_games['currentGameInSeries'] = 1\n",
    "\n",
    "    current_series_game = (team_games.set_index('date')\n",
    "                           .groupby(['teamA', 'teamB', 'season',\n",
    "                                     'seriesDescription'])['currentGameInSeries']\n",
    "                           .expanding().sum())\n",
    "\n",
    "    current_series_game = current_series_game.reset_index()\n",
    "    current_series_game.drop_duplicates(subset=['teamA', 'teamB',\n",
    "                                                'date'], keep='last', inplace=True)\n",
    "    current_series_game.reset_index(drop=True, inplace=True)    \n",
    "    return current_series_game\n",
    "\n",
    "\n",
    "def compute_current_game_in_series_and_join(games: pd.DataFrame):\n",
    "    current_series_game = compute_current_game_in_series(games)\n",
    "    current_series_game.drop(['season', 'seriesDescription'], axis=1, inplace=True)\n",
    "    games = games.merge(current_series_game,\n",
    "                        left_on=['homeId', 'awayId', 'date'],\n",
    "                        right_on=['teamA', 'teamB', 'date'],\n",
    "                        how='left')\n",
    "    \n",
    "    games.drop(['teamA', 'teamB'], axis=1, inplace=True)\n",
    "    games['currentGameInSeries'] = ((games['currentGameInSeries'] - 1) %\n",
    "                                     games['gamesInSeries'])\n",
    "    return games\n",
    "    \n",
    "\n",
    "def compute_games_stats(games: pd.DataFrame) -> pd.DataFrame:\n",
    "    team_games = _join_home_and_away_games(games)\n",
    "    team_games = team_games.loc[:, ['teamA', 'teamB', 'date', 'season',\n",
    "                                    'awayWinner', 'homeWinner', 'home', 'away']]\n",
    "    # fill the nan with False\n",
    "    # this is because you cant win at home when playing as visitant\n",
    "    team_games['homeWinner'] = team_games['homeWinner'] * team_games['home']\n",
    "    team_games['awayWinner'] = team_games['awayWinner'] * team_games['away']\n",
    "    # some day have different days, lets sum over each day\n",
    "    team_games = team_games.groupby(['teamA', 'teamB',  'season', 'date']).sum()\n",
    "    team_games.reset_index(inplace=True)\n",
    "    \n",
    "    # sort values\n",
    "    team_games = (team_games.sort_values(by=['teamA', 'teamB', 'date'])\n",
    "                  .reset_index(drop=True))\n",
    "    # calculate the cummulative sum\n",
    "    team_games = (team_games.set_index('date')\n",
    "                  .groupby(['teamA', 'teamB', 'season'])[['home', 'away', 'homeWinner', 'awayWinner']]\n",
    "                  .expanding().sum())\n",
    "    # compute stats\n",
    "    team_games['totalGamesVsoppTeam'] = (team_games['away'] + team_games['home'])\n",
    "    team_games['WinPctAsHome'] = team_games['homeWinner'] / team_games['home']\n",
    "    team_games['WinPctAsAway'] = team_games['awayWinner'] / team_games['away']\n",
    "    team_games['WintPctHist'] = ((team_games['homeWinner'] + team_games['awayWinner'])\n",
    "                                 /  team_games['totalGamesVsoppTeam'])\n",
    "    # fillnan with 0\n",
    "    team_games.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    team_games.fillna(0, inplace=True)\n",
    "    # reset the index\n",
    "    team_games.reset_index(inplace=True)\n",
    "    team_games.drop(['away', 'home', 'homeWinner', 'awayWinner', 'season'], axis=1, inplace=True)\n",
    "    return team_games\n",
    "\n",
    "\n",
    "def compute_games_stats_and_join(games: pd.DataFrame) -> pd.DataFrame:\n",
    "    input_shape = len(games)\n",
    "    team_games = compute_games_stats(games)\n",
    "    features = ['WinPctAsHome', 'WinPctAsAway', 'WintPctHist']\n",
    "    # merge for the home ids\n",
    "    games = games.merge(team_games, left_on=['homeId', 'awayId', 'date'],\n",
    "            right_on=['teamA', 'teamB', 'date'],\n",
    "            how='left')\n",
    "    # rename columns to start with home\n",
    "    games.drop(['teamA', 'teamB', 'totalGamesVsoppTeam'], inplace=True, axis=1)\n",
    "    games.rename(columns={f: 'home' + f for f in features}, inplace=True)\n",
    "    \n",
    "    # merge for the away teams\n",
    "    games = games.merge(team_games, left_on=['homeId', 'awayId', 'date'],\n",
    "                right_on=['teamB', 'teamA', 'date'],\n",
    "                how='left')\n",
    "    # rename columns to start with waway\n",
    "    games.rename(columns={f: 'away' + f for f in features}, inplace=True)\n",
    "    games.drop(['teamA', 'teamB'], inplace=True, axis=1)\n",
    "    \n",
    "    assert len(games) == input_shape, \\\n",
    "           f'the input lenght (input_shape) != output shape (len(games))'\n",
    "    return games\n",
    "\n",
    "\n",
    "def ingest_games_stats(games: pd.DataFrame):\n",
    "    games = preprocess_games_stats(games)\n",
    "    games = compute_current_game_in_series_and_join(games)\n",
    "    games = compute_games_stats_and_join(games)\n",
    "    games.drop(['season', 'gamesInSeries', 'seriesDescription'], axis=1, inplace=True)\n",
    "    return games\n",
    "\n",
    "def join_games_stats_to_pstats(pstats: pd.DataFrame,\n",
    "                               games: pd.DataFrame):\n",
    "    teamFeatures = ['Id',\n",
    "                    'Winner',\n",
    "                    'Score',\n",
    "                    'WinPctAsHome',\n",
    "                    'WinPctAsAway',\n",
    "                    'WintPctHist']\n",
    "\n",
    "    add_suffix = lambda suffix: [suffix + f for f in teamFeatures] \n",
    "\n",
    "    homeFeatures = add_suffix('home')\n",
    "    awayFeatures = add_suffix('away')\n",
    "    playerTeamFeatures = add_suffix('playerTeam')\n",
    "    opTeamFeatures = add_suffix('opponentTeam')\n",
    "    \n",
    "    pstats = pstats.merge(games, on=['gamePk', 'date'], how='left')\n",
    "    # home and aways stats to player and opponent stats\n",
    "    pstats.loc[:, playerTeamFeatures] = np.where(pstats[['home']], pstats[homeFeatures], pstats[awayFeatures])\n",
    "    pstats.loc[:, opTeamFeatures] = np.where(pstats[['home']], pstats[awayFeatures], pstats[homeFeatures])\n",
    "    # drop features\n",
    "    redundat_features = ['playerTeamId', 'opponentTeamWinner', 'opponentTeamWintPctHist']\n",
    "    pstats.drop((homeFeatures + awayFeatures +\n",
    "                 redundat_features),\n",
    "                axis=1, inplace=True)\n",
    "    # compute the difference between the scores\n",
    "    return pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc4ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/ingest_data/player_stats.py\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "player_stats_features = ['battingOrder', 'gamesPlayedBatting', 'flyOuts',\n",
    "       'groundOuts', 'runsScored', 'doubles', 'triples', 'homeRuns',\n",
    "       'strikeOuts', 'baseOnBalls', 'intentionalWalks', 'hits', 'hitByPitch',\n",
    "       'atBats', 'caughtStealing', 'stolenBases', 'groundIntoDoublePlay',\n",
    "       'groundIntoTriplePlay', 'plateAppearances', 'totalBases', 'rbi',\n",
    "       'leftOnBase', 'sacBunts', 'sacFlies', 'catchersInterference',\n",
    "       'pickoffs', 'gamesPlayedPitching', 'gamesStartedPitching',\n",
    "       'completeGamesPitching', 'shutoutsPitching', 'winsPitching',\n",
    "       'lossesPitching', 'flyOutsPitching', 'airOutsPitching',\n",
    "       'groundOutsPitching', 'runsPitching', 'doublesPitching',\n",
    "       'triplesPitching', 'homeRunsPitching', 'strikeOutsPitching',\n",
    "       'baseOnBallsPitching', 'intentionalWalksPitching', 'hitsPitching',\n",
    "       'hitByPitchPitching', 'atBatsPitching', 'caughtStealingPitching',\n",
    "       'stolenBasesPitching', 'inningsPitched', 'saveOpportunities',\n",
    "       'earnedRuns', 'battersFaced', 'outsPitching', 'pitchesThrown', 'balls',\n",
    "       'strikes', 'hitBatsmen', 'balks', 'wildPitches', 'pickoffsPitching',\n",
    "       'rbiPitching', 'gamesFinishedPitching', 'inheritedRunners',\n",
    "       'inheritedRunnersScored', 'catchersInterferencePitching',\n",
    "       'sacBuntsPitching', 'sacFliesPitching', 'saves', 'holds', 'blownSaves',\n",
    "       'assists', 'putOuts', 'errors', 'chances']\n",
    "\n",
    "\n",
    "players_features_to_drop = ['gamesPlayedBatting', 'flyOuts', 'doubles', 'triples', 'atBats',\n",
    "       'caughtStealing', 'groundIntoDoublePlay', 'leftOnBase', 'sacBunts',\n",
    "       'sacFlies', 'shutoutsPitching', 'flyOutsPitching',\n",
    "       'airOutsPitching', 'doublesPitching',\n",
    "       'triplesPitching', 'homeRunsPitching', 'baseOnBallsPitching',\n",
    "       'intentionalWalksPitching', 'hitsPitching', 'hitByPitchPitching',\n",
    "       'stolenBasesPitching', 'earnedRuns', 'pitchesThrown', 'balls',\n",
    "       'strikes', 'hitBatsmen', 'wildPitches', 'rbiPitching',\n",
    "       'inheritedRunnersScored', 'sacFliesPitching', 'errors']\n",
    "\n",
    "\n",
    "rank_features = ['runsScored', 'homeRuns', 'hits', 'SLG', 'rbi', 'runsPitching']\n",
    "\n",
    "def compute_player_metrics(pstats: pd.DataFrame):\n",
    "    pstats['SLG'] = ((1 * pstats['hits'] +\n",
    "                      2 * pstats['doubles'] +\n",
    "                      3 * pstats['triples'] +\n",
    "                      4 * pstats['homeRuns']) / pstats['atBats']).fillna(-1).to_numpy()\n",
    "    return pstats\n",
    "    \n",
    "to_keep = ['home', 'gamePk', 'playerId', 'date', 'teamId']\n",
    "\n",
    "def preprocess_player_stats(pstats: pd.DataFrame):\n",
    "    # we drop teamId because this info is in roster]\n",
    "    agg_pstats = (pstats.groupby(['playerId', 'date'])[player_stats_features]\n",
    "                  .sum().reset_index())\n",
    "\n",
    "    pstats = (pstats.drop_duplicates(subset=['playerId', 'date'], keep='last')\n",
    "              .loc[:, to_keep])\n",
    "    \n",
    "    pstats['pstatsDate'] = pstats['date'].copy(deep=True)\n",
    "    pstats = pstats.merge(agg_pstats, on=['playerId', 'date'], how='left')\n",
    "    assert not has_duplicates(pstats), 'player stats include duplicates'\n",
    "    return pstats\n",
    "\n",
    "\n",
    "def ingest_player_stats(pstats: pd.DataFrame):\n",
    "    pstats = preprocess_player_stats(pstats)\n",
    "    pstats = compute_player_metrics(pstats)\n",
    "    # rank features per game\n",
    "    # pstats = compute_rank_features(pstats, on=['gamePk'],\n",
    "                                #    features=rank_features)\n",
    "    pstats = normalize_with_max(pstats, on=['date'],\n",
    "                                features=rank_features)\n",
    "    pstats = pstats.drop(players_features_to_drop, axis=1)\n",
    "    stats_features = pstats.columns.drop(to_keep + ['pstatsDate'])\n",
    "    pstats = add_suffix(pstats, features=stats_features, suffix='ptvf')\n",
    "    return pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91e014df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/ingest_data/standings.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def preprocess_standings(standings: pd.DataFrame):\n",
    "    def _streak_code(standings):\n",
    "        standings['streakCode'] = standings['streakCode'].fillna('W0')\n",
    "        streak = standings['streakCode'].str[1:].astype(np.float32)\n",
    "        code = np.where(standings['streakCode'].str[:1] == 'W', 1, -1)\n",
    "        standings['streak'] = streak * code\n",
    "        standings.drop('streakCode', axis=1, inplace=True)\n",
    "    to_drop = ['gameDate', 'teamName',\n",
    "               'leagueGamesBack',\n",
    "               'sportGamesBack', 'divisionGamesBack', \n",
    "               'runsAllowed', 'runsScored', \n",
    "               'extraInningWins', 'extraInningLosses', \n",
    "               'oneRunWins',\n",
    "               'oneRunLosses', 'dayWins', 'dayLosses',\n",
    "               'nightWins', 'nightLosses',\n",
    "               'grassWins', 'grassLosses', 'turfWins',\n",
    "               'turfLosses', 'divWins',\n",
    "               'divLosses', 'alWins', 'alLosses',\n",
    "               'nlWins', 'nlLosses', 'season', \n",
    "               'wins', 'losses', 'wildCardEliminationNumber',\n",
    "               'eliminationNumber', 'divisionRank', 'leagueRank', 'wildCardRank',\n",
    "               'divisionId', 'streakCode']\n",
    "    standings = standings.drop(to_drop, axis=1)\n",
    "    # _streak_code(standings)\n",
    "    standings['wildCardLeader'] = (standings['wildCardLeader'].replace({'None': False})\n",
    "                                   .fillna(False).astype(np.bool_))\n",
    "\n",
    "    bool_features = ['divisionChamp', 'divisionLeader']\n",
    "    standings[bool_features] = standings[bool_features].astype('float')\n",
    "\n",
    "    standings['homeWinPct'] = standings['homeWins'] / (standings['homeWins'] + standings['homeLosses'])\n",
    "    standings['awayWinPct'] = standings['awayWins'] / (standings['awayWins'] + standings['awayLosses'])\n",
    "\n",
    "    standings.rename(columns={'pct': 'winPct'}, inplace=True)\n",
    "    standings = standings.drop(['homeWins', 'homeLosses',\n",
    "                                 'awayWins', 'awayLosses'], axis=1)\n",
    "    # scale up to 1\n",
    "    standings[['lastTenWins', 'lastTenLosses']] /= 10\n",
    "    return standings\n",
    "    \n",
    "\n",
    "def compute_standings_features(standings: pd.DataFrame, features: List[str]):\n",
    "    return compute_rank_features(standings, on=['date'], features=features)\n",
    "\n",
    "\n",
    "def ingest_standings(standings: pd.DataFrame):\n",
    "    standings = preprocess_standings(standings)\n",
    "    standings = compute_standings_features(standings, features=['homeWinPct', 'awayWinPct', 'winPct'])\n",
    "    return standings\n",
    "\n",
    "\n",
    "# def join_standings_to(df: pd.DataFrame,\n",
    "#                       standings: pd.DataFrame,\n",
    "#                       as_away: bool = False):\n",
    "#     team_col = 'teamId' if as_away else 'opponentTeamId'\n",
    "#     output_name = 'playerTeam' if as_away else 'opponentTeam'\n",
    "#     _standings = standings.rename(columns={'teamId': team_col})\n",
    "\n",
    "#     df = df.merge(_standings, how='left', on=['date', team_col])\n",
    "#     df.rename(columns={f: f'{output_name}{f.title()}St'\n",
    "#                            for f in _standings.columns.drop([team_col, 'date'])}, inplace=True)\n",
    "#     return df\n",
    "\n",
    "\n",
    "def join_standings_to(df: pd.DataFrame,\n",
    "                      standings: pd.DataFrame):\n",
    "    def _merge(df: pd.DataFrame, standings, team_col: str, output_name: str):\n",
    "        df = df.merge(standings, how='left', on=['date', team_col])\n",
    "        df.rename(columns={f: f'{output_name}{f.title()}'\n",
    "                            for f in standings.columns.drop([team_col, 'date'])}, inplace=True)\n",
    "        return df\n",
    "\n",
    "    df = _merge(df, standings, team_col='teamId',\n",
    "                output_name='playerTeam')\n",
    "    # if 'opponentTeamId' not in df.columns:\n",
    "        # return df\n",
    "    # standings = standings.rename(columns={'teamId': 'opponentTeamId'})\n",
    "    # df = _merge(df, standings, team_col='opponentTeamId',\n",
    "                # output_name='opponentTeam')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85898d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/data/ingest_data/team_stats.py\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "team_stats_features = ['runsScored', 'homeRuns', 'strikeOuts', 'hits', 'runsPitching',\n",
    "                       'homeRunsPitching', 'outsPitching','rbiPitching']\n",
    "\n",
    "rank_features = ['runsScored', 'homeRuns']\n",
    "\n",
    "def preprocess_teams(teams: pd.DataFrame):\n",
    "    to_keep = ['teamId', 'date']\n",
    "    # we drop teamId because this info is in roster]\n",
    "    agg_teams = (teams.groupby(['teamId', 'date'])[team_stats_features]\n",
    "                  .sum().reset_index())\n",
    "\n",
    "    teams = (teams.drop_duplicates(subset=['teamId', 'date'], keep='last')\n",
    "              .loc[:, to_keep])\n",
    "    \n",
    "    teams = teams.merge(agg_teams, on=['teamId', 'date'], how='left')\n",
    "    assert not has_duplicates(teams, on=['teamId', 'date']), 'team stats include duplicates'\n",
    "    return teams\n",
    "\n",
    "def ingest_team_stats(teams: pd.DataFrame):\n",
    "    teams = preprocess_teams(teams)\n",
    "    teams = normalize_with_max(teams, on=['date'],\n",
    "                                features=rank_features)\n",
    "    return teams\n",
    "\n",
    "def join_team_stats_to_pstats(pstats: pd.DataFrame,\n",
    "                              teams: pd.DataFrame):\n",
    "    features = teams.columns.drop(['date', 'teamId'])\n",
    "    player_features = list('playerTeam' + features)\n",
    "    away_features = list('opponentTeam' + features)\n",
    "    \n",
    "    # for the player team\n",
    "    pstats = pstats.merge(teams, on=['teamId', 'date'], how='left')\n",
    "    pstats.rename(columns={old: new\n",
    "                           for old, new in zip(features, player_features)},\n",
    "                  inplace=True)\n",
    "    \n",
    "    # for the away team\n",
    "    teams.rename(columns={'teamId': 'opponentTeamId'}, inplace=True)\n",
    "    pstats = pstats.merge(teams, on=['opponentTeamId', 'date'], how='left')\n",
    "    pstats.rename(columns={old: new\n",
    "                           for old, new in zip(features, away_features)},\n",
    "                  inplace=True)\n",
    "    \n",
    "    return pstats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96df15cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/anaconda3/envs/mlb/lib/python3.8/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "### src/models/cont_emb_stack.py\n",
    "import pytorch_lightning as pl\n",
    "from typing import List, Tuple, Callable, Dict\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "def mae(yhat, y, weight=None):\n",
    "    error = torch.abs(yhat-y)\n",
    "    if weight is None:\n",
    "        return error.mean()\n",
    "    weight /= weight.sum()\n",
    "    return (error * weight).sum()\n",
    "\n",
    "\n",
    "def get_emb_size(cardinality: int, maximum: int = 20):\n",
    "    emb_szs = int(np.ceil(cardinality**(1/2)))\n",
    "    return (cardinality, min(emb_szs, maximum))\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, categories: Dict[str, int],\n",
    "                 dropout: float = 0.,\n",
    "                 max_emb_sz: int = 20,\n",
    "                 stack_fn: Callable = torch.cat):\n",
    "        super().__init__()\n",
    "\n",
    "        emb_szs = OrderedDict({name: get_emb_size(size, max_emb_sz)\n",
    "                              for name, size in categories.items()})\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.emb = nn.ModuleDict(OrderedDict({name: nn.Embedding(size, hidden_dim)\n",
    "                                             for name, (size, hidden_dim) in emb_szs.items()}))\n",
    "        self.stack_fn = stack_fn\n",
    "        self.out_features = sum([hidden_dim for (_, hidden_dim) in emb_szs.values()])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.stack_fn([emb(x[:, e]) for e, emb in enumerate(self.emb.values())], dim=-1)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def build_feed_forward(layers: List[int],\n",
    "                       use_bn: bool = False,\n",
    "                       input_bn: bool = False,\n",
    "                       act_fn: Callable = nn.ReLU,\n",
    "                       dropout: List[int] = None):\n",
    "    model = []\n",
    "\n",
    "    if input_bn:\n",
    "        model.append(nn.BatchNorm1d(layers[0]))\n",
    "\n",
    "    if isinstance(dropout, float):\n",
    "        dropout = [dropout] * len(layers)\n",
    "\n",
    "    for L in range(len(layers)-1):\n",
    "        input_dim, output_dim = layers[L], layers[L+1]\n",
    "        linear = nn.Linear(input_dim, output_dim)\n",
    "        model.append(linear)\n",
    "        if L+1 < len(layers) -1:\n",
    "            model.append(act_fn())\n",
    "            if use_bn:\n",
    "                model.append(nn.BatchNorm1d(output_dim))\n",
    "            if dropout is not None:\n",
    "                model.append(nn.Dropout(dropout[L]))\n",
    "\n",
    "    return nn.Sequential(*model)\n",
    "\n",
    "\n",
    "def compute_factor_layers(input_dim,\n",
    "                          depth: int = 1,\n",
    "                          factor: float = 1.,\n",
    "                          dtype=np.int64):\n",
    "    layers_sizes = np.array([input_dim] * (depth-1))\n",
    "    layers_pct = np.array([[factor] * (depth-1)])\n",
    "    layers_pct = np.cumprod(layers_pct)\n",
    "    layers_sizes = list((layers_sizes * layers_pct).astype(dtype))\n",
    "    return layers_sizes\n",
    "\n",
    "\n",
    "def scale_to_100(x):\n",
    "    return torch.sigmoid(x) * 100\n",
    "\n",
    "def log_softmax(x):\n",
    "    return torch.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class EmbModelModule(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 cont_features: int,\n",
    "                 categories: Dict[str, int],\n",
    "                 encoder_dim: int,\n",
    "                 depth: int,\n",
    "                 decrease_factor: float = 1.,\n",
    "                 drop_decrease_factor: float = 1.,\n",
    "                 emb_dropout: float = 0.,\n",
    "                 max_emb_sz: int = 20,\n",
    "                 dropout: float = 0.,\n",
    "                 lr: float = 0.01,\n",
    "                 wd: float = 0.,\n",
    "                 out_features: int = 1,\n",
    "                 output_act_fn: Callable = None):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.output_act_fn = output_act_fn\n",
    "        \n",
    "        self.emb = EmbeddingLayer(categories, dropout=emb_dropout, max_emb_sz=max_emb_sz)\n",
    "        \n",
    "        layers_sizes = compute_factor_layers(encoder_dim, depth=depth, factor=decrease_factor)\n",
    "        drop_sizes = compute_factor_layers(dropout, depth=depth,\n",
    "                                           factor=drop_decrease_factor, dtype=float)\n",
    "        daily_features = self.emb.out_features + cont_features\n",
    "        dropout_layers = [dropout] + drop_sizes\n",
    "        daily_decoder_layers = [daily_features, encoder_dim] + layers_sizes + [out_features]\n",
    "        self.output_layer = build_feed_forward(daily_decoder_layers, use_bn=False,\n",
    "                                                dropout=dropout_layers,\n",
    "                                                act_fn=self._act_fn)\n",
    "\n",
    "    def _act_fn(self):\n",
    "        return nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, features,\n",
    "                categories,\n",
    "                target=None,\n",
    "                weight=None):\n",
    "\n",
    "        categories = self.emb(categories)\n",
    "\n",
    "        features = torch.cat((features, categories), dim=-1)\n",
    "        \n",
    "        prediction = self.output_layer(features)\n",
    "        if self.output_act_fn is not None:\n",
    "            prediction = self.output_act_fn(prediction)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class RegressionEmbModel(EmbModelModule):\n",
    "    def __init__(self, \n",
    "                 cont_features: int,\n",
    "                 categories: Dict[str, int],\n",
    "                 encoder_dim: int,\n",
    "                 depth: int,\n",
    "                 decrease_factor: float = 1.,\n",
    "                 drop_decrease_factor: float = 1.,\n",
    "                 emb_dropout: float = 0.,\n",
    "                 max_emb_sz: int = 20,\n",
    "                 dropout: float = 0.,\n",
    "                 lr: float = 0.01,\n",
    "                 wd: float = 0.,\n",
    "                 out_features: int = 1,\n",
    "                 scale_output: bool = True):\n",
    "\n",
    "        output_act_fn = (scale_to_100 if scale_output else None)\n",
    "        super().__init__(cont_features=cont_features, categories=categories,\n",
    "                         max_emb_sz=max_emb_sz, encoder_dim=encoder_dim, out_features=out_features,\n",
    "                         dropout=dropout, emb_dropout=emb_dropout, depth=depth, lr=lr,\n",
    "                         decrease_factor=decrease_factor, drop_decrease_factor=drop_decrease_factor,\n",
    "                         wd=wd, output_act_fn=output_act_fn)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self(**batch)\n",
    "        weight = batch['weight'] if 'weight' in batch else None\n",
    "        loss = mae(y_hat, batch['target'], weight)\n",
    "        self.log('train_loss', loss, on_epoch=True,\n",
    "                 prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self(**batch)\n",
    "        loss = mae(y_hat, batch['target'])\n",
    "        self.log('valid_loss', loss, on_epoch=True,\n",
    "                 prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "class ClassificationEmbModel(EmbModelModule):\n",
    "    def __init__(self, \n",
    "                 cont_features: int,\n",
    "                 categories: Dict[str, int],\n",
    "                 encoder_dim: int,\n",
    "                 depth: int,\n",
    "                 decrease_factor: float = 1.,\n",
    "                 drop_decrease_factor: float = 1.,\n",
    "                 emb_dropout: float = 0.,\n",
    "                 max_emb_sz: int = 20,\n",
    "                 dropout: float = 0.,\n",
    "                 lr: float = 0.01,\n",
    "                 wd: float = 0.,\n",
    "                 out_features: int = 1,\n",
    "                 pos_weight: torch.tensor = None):\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "        output_act_fn = log_softmax\n",
    "        super().__init__(cont_features=cont_features, categories=categories,\n",
    "                         max_emb_sz=max_emb_sz, encoder_dim=encoder_dim, out_features=out_features,\n",
    "                         dropout=dropout, emb_dropout=emb_dropout, depth=depth, lr=lr,\n",
    "                         decrease_factor=decrease_factor, drop_decrease_factor=drop_decrease_factor,\n",
    "                         wd=wd, output_act_fn=output_act_fn)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self(**batch)\n",
    "        loss = F.nll_loss(y_hat, batch['target'], weight=self.pos_weight)\n",
    "        self.log('train_loss', loss, on_epoch=True,\n",
    "                 prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self(**batch)\n",
    "        loss = F.nll_loss(y_hat, batch['target'], weight=self.pos_weight)\n",
    "        self.log('valid_loss', loss, on_epoch=True,\n",
    "                 prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8ff2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/models/lstm.py\n",
    "import pytorch_lightning as pl\n",
    "from typing import List, Tuple, Callable, Dict\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "def mae(yhat, y):\n",
    "    return torch.abs(yhat-y).mean()\n",
    "\n",
    "def get_emb_size(cardinality: int, maximum: int = 20):\n",
    "    return (cardinality+1, min(int(cardinality**(1/2)), maximum))\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, categories: Dict[str, int],\n",
    "                 dropout: float = 0.,\n",
    "                 max_emb_sz: int = 20,\n",
    "                 stack_fn: Callable = torch.cat):\n",
    "        super().__init__()\n",
    "\n",
    "        emb_szs = OrderedDict({name: get_emb_size(size, max_emb_sz)\n",
    "                              for name, size in categories.items()})\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.emb = nn.ModuleDict(OrderedDict({name: nn.Embedding(size, hidden_dim)\n",
    "                                             for name, (size, hidden_dim) in emb_szs.items()}))\n",
    "        self.stack_fn = stack_fn\n",
    "        self.out_features = sum([hidden_dim for (_, hidden_dim) in emb_szs.values()])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.stack_fn([emb(x[:, e]) for e, emb in enumerate(self.emb.values())], dim=-1)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LstmModel(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 static_features: int,\n",
    "                 time_features: int,\n",
    "                 categories: Dict[str, int],\n",
    "                 max_emb_sz: int,\n",
    "                 hidden_dim: int,\n",
    "                 encoder_dim: int,\n",
    "                 emb_dropout: float = 0.,\n",
    "                 dropout: float = 0.,\n",
    "                 lr: float = 0.01,\n",
    "                 wd: float = 0.,\n",
    "                 out_features: int = 1,\n",
    "                 n_layers: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        \n",
    "        self.emb = EmbeddingLayer(categories, dropout=emb_dropout, max_emb_sz=max_emb_sz)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # lstm\n",
    "        self.net = nn.LSTM(time_features, hidden_dim, batch_first=True)\n",
    "        # decoder\n",
    "        input_decoder = static_features + hidden_dim + self.emb.out_features\n",
    "        self.output_layer = nn.Sequential(nn.Linear(input_decoder, encoder_dim),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.BatchNorm1d(encoder_dim),\n",
    "                                          nn.Dropout(dropout),\n",
    "                                          nn.Linear(encoder_dim, out_features))\n",
    "\n",
    "    def _init_hidden(self, bs):\n",
    "        return next(self.parameters()).new(1, bs, self.hidden_dim).zero_()\n",
    "        \n",
    "    def init_hidden_state(self, bs):\n",
    "        return (self._init_hidden(bs), self._init_hidden(bs))\n",
    "    \n",
    "    def forward(self, features,\n",
    "                timeft,\n",
    "                categories,\n",
    "                target=None):        \n",
    "        bs, sq, ft = timeft.size()\n",
    "        hidden_state = self.init_hidden_state(bs)\n",
    "\n",
    "        _, (final_state, _) = self.net(timeft, hidden_state)\n",
    "        \n",
    "        final_state.squeeze_(dim=0)\n",
    "        categories = self.emb(categories)\n",
    "        final_state = torch.cat((features, categories, final_state), dim=1)\n",
    "        prediction = self.output_layer(final_state)\n",
    "        # scaled prediction to 0 to 100\n",
    "        prediction = torch.sigmoid(prediction) * 100\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self(**batch)\n",
    "        loss = mae(y_hat, batch['target'])\n",
    "        self.log('train_mae', loss, on_epoch=True,\n",
    "                 prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self(**batch)\n",
    "        loss = mae(y_hat, batch['target'])\n",
    "        self.log('valid_mae', loss, on_epoch=True,\n",
    "                 prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7456f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/train/cont_emb_stack.py\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "def predict_dl(model: nn.Module, valid_dl: DataLoader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        prediction = torch.cat([model(**batch)\n",
    "                               for batch in valid_dl])\n",
    "    return prediction.numpy()\n",
    "\n",
    "\n",
    "def load_best_state(model, checkpoint_callback):\n",
    "    print('loading model to best score')\n",
    "    print(f'best score = {checkpoint_callback.best_model_score}')\n",
    "    best_model_parameters = torch.load(checkpoint_callback.best_model_path)['state_dict']\n",
    "    model.load_state_dict(best_model_parameters)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def run_fn(config: Dict[str, Any],\n",
    "           train_data: pd.DataFrame,\n",
    "           valid_data: pd.DataFrame):\n",
    "    \n",
    "    if config.seed is not None:\n",
    "        torch.manual_seed(config.seed)\n",
    "\n",
    "    # hyperparameters\n",
    "    hp = config.hp\n",
    "    \n",
    "    # create dls\n",
    "    train_ds = PlayerDataset.from_df(train_data,\n",
    "                                      features=config.features,\n",
    "                                      categories=config.categories,\n",
    "                                      target=config.target_cols,\n",
    "                                      weight=config.weight,\n",
    "                                      bptt=0)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=hp.batch_size, shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "    valid_ds = PlayerDataset.from_df(valid_data, \n",
    "                                      features=config.features,\n",
    "                                      categories=config.categories,\n",
    "                                      target=config.target_cols,\n",
    "                                      bptt=0)\n",
    "\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=hp.batch_size,\n",
    "                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    categories = train_data.loc[:, config.categories].nunique().to_dict()\n",
    "\n",
    "    model = RegressionEmbModel(cont_features=len(config.features),\n",
    "                      categories=categories,\n",
    "                      max_emb_sz=hp.max_emb_sz,\n",
    "                      encoder_dim=hp.encoder_dim,\n",
    "                      out_features=len(config.target_cols),\n",
    "                      dropout=hp.dropout,\n",
    "                      emb_dropout=hp.emb_dropout,\n",
    "                      depth=hp.depth,\n",
    "                      lr=hp.lr,\n",
    "                      decrease_factor=hp.decrease_factor,\n",
    "                      drop_decrease_factor=hp.drop_decrease_factor,\n",
    "                      scale_output=hp.scale_output,\n",
    "                      wd=hp.wd)\n",
    "    print(model)\n",
    "\n",
    "    patience = hp.early_stop_patience if hp.early_stop_patience is not None else 3\n",
    "    early_stopping = EarlyStopping('valid_loss', patience=patience)\n",
    "    checkpoint_callback  = ModelCheckpoint(monitor='valid_loss',\n",
    "                                           save_top_k=3,\n",
    "                                           save_weights_only=True)\n",
    "    trainer = pl.Trainer(max_epochs=hp.epochs,\n",
    "                         callbacks=[early_stopping, checkpoint_callback])\n",
    "\n",
    "    trainer.fit(model, train_dl, valid_dl)\n",
    "\n",
    "    # loading best model so far\n",
    "    model = load_best_state(model, checkpoint_callback)\n",
    "    \n",
    "    def predict_fn(test_features: pd.DataFrame):\n",
    "        test_ds = PlayerDataset.from_df(test_features,\n",
    "                                        features=config.features,\n",
    "                                        categories=config.categories,\n",
    "                                        bptt=1)\n",
    "        test_dl = DataLoader(test_ds, batch_size=hp.batch_size, shuffle=False)\n",
    "        prediction = predict_dl(model, test_dl)\n",
    "        del test_dl, test_ds\n",
    "        return prediction\n",
    "\n",
    "    valid_prediction = predict_dl(model, valid_dl)\n",
    "    gc.collect()\n",
    "    return ModelOutput(model, predict_fn, valid_prediction)\n",
    "\n",
    "\n",
    "def run_classification_fn(config: Dict[str, Any],\n",
    "                          train_data: pd.DataFrame,\n",
    "                          valid_data: pd.DataFrame):\n",
    "    \n",
    "    if config.seed is not None:\n",
    "        torch.manual_seed(config.seed)\n",
    "\n",
    "    # hyperparameters\n",
    "    hp = config.hp\n",
    "    \n",
    "    # create dls\n",
    "    train_ds = PlayerDataset.from_df(train_data,\n",
    "                                      features=config.features,\n",
    "                                      categories=config.categories,\n",
    "                                      target=config.target_name,\n",
    "                                      bptt=0)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=hp.batch_size, shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "    valid_ds = PlayerDataset.from_df(valid_data, \n",
    "                                      features=config.features,\n",
    "                                      categories=config.categories,\n",
    "                                      target=config.target_name,\n",
    "                                      bptt=0)\n",
    "\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=hp.batch_size,\n",
    "                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    categories = train_data.loc[:, config.categories].nunique().to_dict()\n",
    "    output_features = train_data[config.target_name].nunique()\n",
    "    \n",
    "    pos_weight = train_data[config.target_name].value_counts()\n",
    "    pos_weight = torch.from_numpy((pos_weight.min() / pos_weight)\n",
    "                                   .to_numpy().astype(np.float32))\n",
    "\n",
    "    model = ClassificationEmbModel(cont_features=len(config.features),\n",
    "                                   categories=categories,\n",
    "                                   max_emb_sz=hp.max_emb_sz,\n",
    "                                   encoder_dim=hp.encoder_dim,\n",
    "                                   out_features=output_features,\n",
    "                                   dropout=hp.dropout,\n",
    "                                   emb_dropout=hp.emb_dropout,\n",
    "                                   depth=hp.depth,\n",
    "                                   lr=hp.lr,\n",
    "                                   decrease_factor=hp.decrease_factor,\n",
    "                                   drop_decrease_factor=hp.drop_decrease_factor,\n",
    "                                   wd=hp.wd, \n",
    "                                   pos_weight=pos_weight)\n",
    "    print(model)\n",
    "\n",
    "    patience = hp.early_stop_patience if hp.early_stop_patience is not None else 3\n",
    "    early_stopping = EarlyStopping('valid_loss', patience=patience)\n",
    "    checkpoint_callback  = ModelCheckpoint(monitor='valid_loss',\n",
    "                                           save_top_k=3,\n",
    "                                           save_weights_only=True)\n",
    "    trainer = pl.Trainer(max_epochs=hp.epochs,\n",
    "                         callbacks=[early_stopping, checkpoint_callback])\n",
    "\n",
    "    trainer.fit(model, train_dl, valid_dl)\n",
    "\n",
    "    # loading best model so far\n",
    "    model = load_best_state(model, checkpoint_callback)\n",
    "    \n",
    "    def predict_fn(test_features: pd.DataFrame):\n",
    "        test_ds = PlayerDataset.from_df(test_features,\n",
    "                                        features=config.features,\n",
    "                                        categories=config.categories,\n",
    "                                        bptt=1)\n",
    "        test_dl = DataLoader(test_ds, batch_size=hp.batch_size, shuffle=False)\n",
    "        prediction = predict_dl(model, test_dl)\n",
    "        del test_dl, test_ds\n",
    "        return prediction\n",
    "\n",
    "    valid_prediction = predict_dl(model, valid_dl)\n",
    "    gc.collect()\n",
    "    return ModelOutput(model, predict_fn, valid_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6067b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/train/core.py\n",
    "from typing import Callable, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from typing import List, Dict, Callable\n",
    "from sklearn.pipeline import Pipeline\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    model: Any\n",
    "    predict_fn: Callable\n",
    "    prediction: np.ndarray\n",
    "\n",
    "\n",
    "def ensemble_pred(preds: List[np.ndarray]):\n",
    "    return np.stack(preds, axis=1).mean(axis=1)\n",
    "\n",
    "class Ensemble():\n",
    "    def __init__(self, models: List[ModelOutput],\n",
    "                 pipeline: Pipeline):\n",
    "        self.models = models\n",
    "        self.pipeline = pipeline\n",
    "    \n",
    "    def __call__(self, raw_df: pd.DataFrame):\n",
    "        test_features = self.pipeline.transform(raw_df)\n",
    "        return ensemble_pred([_model.predict_fn(test_features)\n",
    "                              for _model in self.models])\n",
    "\n",
    "\n",
    "def predict_recursive(test_df: pd.DataFrame,\n",
    "                      raw_df: pd.DataFrame, \n",
    "                      predict_fn: Callable,\n",
    "                      n_days: int,\n",
    "                      target_cols: List[str]):\n",
    "    \n",
    "    assert test_df['date'].nunique() == 1, \\\n",
    "           'the test set has more than one date'\n",
    "\n",
    "    test_date = test_df['date'].iloc[0]\n",
    "    last_date = raw_df['date'].max()\n",
    "    \n",
    "    raw_df = raw_df[raw_df['date'] >= (last_date - pd.to_timedelta(n_days, unit='d'))]\n",
    "    \n",
    "    if test_date <= last_date:\n",
    "        print('test date in training data')\n",
    "        raw_df = raw_df[raw_df['date'] < test_date]\n",
    "        last_date = raw_df['date'].max()\n",
    "    assert test_date - last_date == pd.to_timedelta(1, unit='d'), \\\n",
    "        f'the test date ({test_date}) must be one day after the last_date ({last_date})'\n",
    "    # append information\n",
    "    raw_df = raw_df.append(test_df, ignore_index=True)\n",
    "    # sort by index and dates\n",
    "    raw_df.sort_values(by=['playerId', 'date'], inplace=True)\n",
    "    raw_df.reset_index(drop=True, inplace=True)\n",
    "    # get the index to locate the test df\n",
    "    index = (raw_df['date'] == test_date)\n",
    "    \n",
    "    assert index.sum() == len(test_df)\n",
    "    # predict\n",
    "    prediction = predict_fn(raw_df)\n",
    "    # filter prediction\n",
    "    prediction = prediction[index]\n",
    "    # create a dataframe with the prediction\n",
    "    prediction_df = pd.DataFrame(prediction, columns=target_cols)\n",
    "    prediction_df['date'] = test_date\n",
    "    prediction_df['playerId'] = raw_df.loc[index, 'playerId'].to_numpy()\n",
    "    print(prediction_df.head())\n",
    "    # add the prediction to the dataset\n",
    "    raw_df.loc[index, target_cols] = prediction\n",
    "    \n",
    "    del prediction, index\n",
    "    gc.collect()\n",
    "    return prediction_df, raw_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21c0e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/train/lgbm.py\n",
    "from lightgbm import LGBMRegressor\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "def run_lgbm(config: Dict[str, Any],\n",
    "             train_data: pd.DataFrame,\n",
    "             valid_data:  pd.DataFrame, verbose=100):\n",
    "\n",
    "    models = []\n",
    "    train_features = train_data.loc[:, config.features]\n",
    "    valid_features = valid_data.loc[:, config.features]\n",
    "\n",
    "    for target_name in sorted(config.target_cols):\n",
    "        print(target_name)\n",
    "        _model = LGBMRegressor(**config.hp)\n",
    "        _model.fit(train_features,\n",
    "                   train_data.loc[:, target_name],\n",
    "                   eval_set=[(valid_features, valid_data.loc[:, target_name])],  \n",
    "                    early_stopping_rounds=verbose, \n",
    "                    verbose=verbose,\n",
    "                    categorical_feature=config.categories)\n",
    "        models.append( _model)\n",
    "\n",
    "    def predict_fn(test_features: pd.DataFrame):\n",
    "        return np.stack([_model.predict(test_features.loc[:, config.features])\n",
    "                         for _model in models], axis=1)\n",
    "\n",
    "    valid_prediction = predict_fn(valid_data)\n",
    "\n",
    "    del train_features, valid_features\n",
    "    gc.collect()\n",
    "    return ModelOutput(models, predict_fn, valid_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32385ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### src/train/lstm.py\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "def predict_dl(model: nn.Module, valid_dl: DataLoader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.cat([model(**batch)\n",
    "                               for batch in valid_dl])\n",
    "    return prediction.numpy()\n",
    "\n",
    "\n",
    "def run_lstm(config: Dict[str, Any],\n",
    "             train_data: pd.DataFrame,\n",
    "             valid_data: pd.DataFrame):\n",
    "    \n",
    "    if config.seed is not None:\n",
    "        torch.manual_seed(config.seed)\n",
    "\n",
    "    # hyperparameters\n",
    "    hp = config.hp\n",
    "    \n",
    "    # create dls\n",
    "    train_ds = PlayerDataset.from_df(train_data,\n",
    "                                      features=config.static_features,\n",
    "                                      time_features=config.time_features,\n",
    "                                      categories=config.categories,\n",
    "                                      target=config.target_cols,\n",
    "                                      bptt=hp.bptt)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=hp.batch_size, shuffle=True,\n",
    "                          num_workers=4)\n",
    "    \n",
    "    valid_ds = PlayerDataset.from_df(valid_data, \n",
    "                                      features=config.static_features,\n",
    "                                      time_features=config.time_features,\n",
    "                                      categories=config.categories,\n",
    "                                      target=config.target_cols,\n",
    "                                      bptt=hp.bptt)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=hp.batch_size,\n",
    "                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    \n",
    "    categories = train_data.loc[:, config.categories].nunique().to_dict()\n",
    "    \n",
    "    model = LstmModel(static_features=len(config.static_features),\n",
    "                      time_features=len(config.time_features),\n",
    "                      categories=categories,\n",
    "                      max_emb_sz=hp.max_emb_sz,\n",
    "                      hidden_dim=hp.hidden_dim,\n",
    "                      encoder_dim=hp.encoder_dim,\n",
    "                      out_features=len(config.target_cols),\n",
    "                      dropout=hp.dropout,\n",
    "                      emb_dropout=hp.emb_dropout,\n",
    "                      lr=hp.lr,\n",
    "                      wd=hp.wd)\n",
    "    print(model)\n",
    "    early_stopping = EarlyStopping('valid_mae', patience=3)\n",
    "    trainer = pl.Trainer(max_epochs=hp.epochs, callbacks=[early_stopping])\n",
    "\n",
    "    trainer.fit(model, train_dl, valid_dl)\n",
    "    \n",
    "    def predict_fn(test_features: pd.DataFrame):\n",
    "        test_ds = PlayerDataset.from_df(test_features,\n",
    "                                        features=config.static_features,\n",
    "                                        time_features=config.time_features,\n",
    "                                        categories=config.categories,\n",
    "                                        bptt=hp.bptt)\n",
    "        test_dl = DataLoader(test_ds, batch_size=hp.batch_size, shuffle=False)\n",
    "        prediction = predict_dl(model, test_dl)\n",
    "        del test_dl, test_ds\n",
    "        return prediction\n",
    "\n",
    "    valid_prediction = predict_dl(model, valid_dl)\n",
    "\n",
    "    del valid_dl, train_dl, train_ds, valid_ds, trainer\n",
    "    gc.collect()\n",
    "    return ModelOutput(model, predict_fn, valid_prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
